{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FMurray/hyperdemocracy/blob/main/hyper_democracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KkXxD28y6u2w"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on a google colab, uncomment the lines below to fetch the requirements file and the hyperdemocracy.py module\n",
    "# and pip install the requirements\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/FMurray/hyperdemocracy/main/requirements.txt\n",
    "#!wget https://raw.githubusercontent.com/FMurray/hyperdemocracy/main/hyperdemocracy.py\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rich\n",
    "import time\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose a Provider\n",
    "\n",
    "We have options for HuggingFace and OpenAI model providers in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROVIDER = \"HF\"\n",
    "PROVIDER = \"OPENAI\"\n",
    "\n",
    "assert PROVIDER in [\"HF\", \"OPENAI\"]\n",
    "\n",
    "CONFIGS = {\n",
    "    \"HF\": {\n",
    "        \"embd\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"llm\": \"google/flan-t5-base\",\n",
    "        #\"llm\": \"google/flan-t5-large\",\n",
    "        #\"llm\": \"google/flan-ul2\",\n",
    "    },\n",
    "    \"OPENAI\": {\n",
    "        \"embd\": \"text-embedding-ada-002\",\n",
    "        \"llm\": \"gpt-3.5-turbo-16k\",\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG = CONFIGS[PROVIDER]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Formatted Output\n",
    "\n",
    "Note that we patch the builtin Python `print` function with `rich.print` in the cell below. If you prefer a more traditional print output you can comment out the import below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Cost (OpenAI Only, HF is Free)\n",
    "\n",
    "Running this notebook with your OpenAI key in an environment variable will charge a small amount of money to your OpenAI account. The total cost of running this notebook multiple times should be less than $5 but that can change if the datasource is changed. Each cell that makes a request to an OpenAI endpoint that costs money will have the following comment in it, \n",
    "\n",
    "```\n",
    "## THIS CELL SPENDS MONEY ##\n",
    "```\n",
    "\n",
    "Up to date pricing information on OpenAI models can be found here https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DOLLARS_PER_1K_TOKENS = 0.0001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqdS7Elhfpl"
   },
   "source": [
    "# Setup Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use local secrets, add a file called .env to this directory and uncomment the lines below\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtSzpF4Ehii5"
   },
   "outputs": [],
   "source": [
    "# if you are using google colab, uncomment the lines below to manually enter your OpenAI key.\n",
    "\n",
    "#import getpass\n",
    "#os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using google colab, uncomment the lines below to manually enter your HuggingFace token.\n",
    "\n",
    "#import getpass\n",
    "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for development\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vSe6gY8_gazO"
   },
   "source": [
    "# Load Assembleco Records\n",
    "\n",
    "We are going to use a small subset of records provided by https://assembled.app/.\n",
    "\n",
    "For the purposes of this workshop, we have created a [huggingface dataset](https://huggingface.co/datasets/assembleco/hyperdemocracy)  which we can load using the `load_dataset` function. This is all handled for you in the `load_assembleco_records` function. See more info here [datasets](https://huggingface.co/docs/datasets/index) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperdemocracy import load_assembleco_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_assembleco_records(process=True, strip_html=True, remove_empty_body=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperdemocracy import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5r6oJ7MmQM5"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4eSYiO5Ii9X"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-HUxJ4XAFOaZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sponsor Graph Sidequest\n",
    "\n",
    "We will be focusing on the text content of the legislation in this workshop, but if you would like to explore building a graph from the sponsor / co-sponsor / legislation network check out the [sponsor_graph notebook](https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/sponsor_graph.ipynb) to get started."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5GM8DqH7Ojeu"
   },
   "source": [
    "# From Pandas Dataframe to LangChain Documents\n",
    "\n",
    "A langchain document is a simple class with two attributes, \n",
    "* page_content (a string)\n",
    "* metadata (a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH58_PTsRJyD"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take each row from our legislation DataFrame and create a LangChain Document. We use the `body` column for the `page_content` attribute and populate the `metadata` attribute with data from some of the other columns. Note that the `source` key in the `metadata` dictionary is associated with a congress.gov url. The `source` key can hold an arbitrary string and will become important when we look into question answering systems that return information about the sources used to answer a question. We also restrict ourselves to `str`, `int`, and `float` types in the other values of our `metadata` dictionary. This is to make it easy to use them as filters when querying our vectorstore. If that doesn't make sense, dont worry! It will by the end of the workshop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to embed all the assembled records, you can filter using a search query here:\n",
    "\n",
    "from hyperdemocracy import filter_aco_df\n",
    "query = \"energy\"\n",
    "df = filter_aco_df(df, query)\n",
    "\n",
    "# if that doesn't work for your needs, you can just take a random sample of some number\n",
    "#df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLHx06mHepKQ"
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for irow, row in df.iterrows():\n",
    "    doc = Document(\n",
    "        page_content=row['body'],\n",
    "        metadata={\n",
    "            # Note: chroma can only filter on float, str, or int\n",
    "            # https://docs.trychroma.com/usage-guide#using-where-filters\n",
    "            'key': row['key'],\n",
    "            'congress_num': row['congress_num'],\n",
    "            'legis_class': row['legis_class'],\n",
    "            'legis_num': row['legis_num'],\n",
    "            'name': row['name'],\n",
    "            'summary': row['summary'],\n",
    "            'sponsor': row['sponsors'][0][0],\n",
    "            'source': row['congress_gov_url'],\n",
    "        },\n",
    "    )\n",
    "    all_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-fFRGsTUDGX"
   },
   "outputs": [],
   "source": [
    "print(all_docs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* examine the Document content\n",
    "* visit the congress.gov URL and view the document in various formats\n",
    "* examine the body text below\n",
    "* read the summary of the document and attempt to connect it with the long form text of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fceFBIiDRcZk"
   },
   "outputs": [],
   "source": [
    "print(all_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siRiWjCyj63F"
   },
   "outputs": [],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the token counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBZermt7U07_"
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_openai_tokens_in_docs(docs, model_name=CONFIG[\"embd\"]):\n",
    "    num_tokens = 0\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    for doc in docs:\n",
    "        num_tokens += len(enc.encode(doc.page_content))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hf_tokens_in_docs(docs, model_name=CONFIG[\"embd\"]):\n",
    "    num_tokens = 0\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"embd\"])\n",
    "    for doc in docs:\n",
    "        num_tokens += len(tokenizer(docs[0].page_content)['input_ids'])\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate cost\n",
    "if PROVIDER == \"OPENAI\":\n",
    "    num_tokens = count_openai_tokens_in_docs(docs)\n",
    "    cost = EMBED_DOLLARS_PER_1K_TOKENS * num_tokens / 1000\n",
    "    print('Num Docs: ', len(docs))\n",
    "    print('Num Tokens: ', num_tokens)\n",
    "    print('Total Cost (USD): ', '$'+str(cost))\n",
    "elif PROVIDER == \"HF\":\n",
    "    num_tokens = count_hf_tokens_in_docs(docs)\n",
    "    cost = 0\n",
    "    print('Num Docs: ', len(docs))\n",
    "    print('Num Tokens: ', num_tokens)\n",
    "    print('Total Cost (USD): ', '$'+str(cost))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* Contemplate why the answers are slightly different between the \"QA\" result and the \"QA with sources\" result.\n",
    "* Visit the source links and check if the linked legislation is relevant to the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4ry8zdrlCEqi"
   },
   "source": [
    "# Document QA - Step by Step\n",
    "\n",
    "Our goal is to setup a question answering (QA) system that can repond to natural language questions about legislation using source material that we provide. In the following sections we will unpack all of components and go over them in detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KCdDA0eOXJ_1"
   },
   "source": [
    "# Part 1 - Langchain Text Splitters\n",
    "\n",
    "> When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "> At a high level, text splitters work as following:\n",
    "\n",
    ">    1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    ">    2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    ">    3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "> That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    ">    1. How the text is split\n",
    ">    2. How the chunk size is measured\n",
    "\n",
    "-- https://python.langchain.com/docs/modules/data_connection/document_transformers/#text-splitters\n",
    "\n",
    "Here are some useful options for splitting legislative text, \n",
    "\n",
    "* [character text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)\n",
    "  * How the text is split: by single character\n",
    "  * How the chunk size is measured: by number of characters\n",
    "* [recursive text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
    "  * How the text is split: by list of characters\n",
    "  * How the chunk size is measured: by number of characters\n",
    "* [split by token](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token)\n",
    "  * How the text is split: by character passed in\n",
    "  * How the chunk size is measured: by tiktoken tokenizer\n",
    "\n",
    "If you are not familiar with the concept of a token, this article may help, \n",
    "* https://simonwillison.net/2023/Jun/8/gpt-tokenizers/\n",
    "\n",
    "## Side Quest\n",
    "* check out the [text splitting notebook](https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/text_splitting.ipynb) side quest to see more details on text splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w6e8Bcia6gp"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Make a TextSplitter Choice here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCi59g8rFAMi"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da7F2FUuFR8q"
   },
   "outputs": [],
   "source": [
    "print(\"Number of original docs: \", len(docs))\n",
    "print(\"Number of split docs: \", len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_docs[50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVOa0TebHtY"
   },
   "source": [
    "# Part 2 - Embed and Index Doc Chunks\n",
    "\n",
    "Now we will embed and index the document chunks from the previous section. \n",
    "We have many choices when it comes to text embedding models and vector indexes. \n",
    "For this tutorial we will choose, \n",
    "\n",
    "* text embedding model: `text_embedding_ada_002`\n",
    "* vector index:\n",
    "  * https://www.trychroma.com\n",
    "  * https://docs.trychroma.com/usage-guide#changing-the-distance-function\n",
    "  * https://github.com/nmslib/hnswlib/tree/master#supported-distances\n",
    "  * https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/chroma.py\n",
    "  * https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/utils.py#L10\n",
    "\n",
    "For a look at some of the top performing closed and open source text embedding models, check out the HuggingFace Massive Text Embedding Benchmark (MTEB), \n",
    "* https://huggingface.co/spaces/mteb/leaderboard\n",
    "  \n",
    "For a more detailed introduction to embeddings in general, see the embeddings notebook\n",
    "* https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdBDoTvLbLfZ"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROVIDER == \"HF\":\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=CONFIG[\"embd\"])\n",
    "elif PROVIDER == \"OPENAI\":\n",
    "    embeddings = OpenAIEmbeddings(model=CONFIG[\"embd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = f\"hyperdemocracy-chromadb-prov-{PROVIDER}-ndocs-{NUM_DOCS}\"\n",
    "print(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18-prZMJFisk"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY THE FIRST TIME ##\n",
    "if os.path.exists(persist_directory):\n",
    "    vec_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "else:\n",
    "    batch_size = 128\n",
    "    for ii in tqdm(range(0, len(split_docs), batch_size)):\n",
    "        batch = split_docs[ii:ii+batch_size]\n",
    "        if ii == 0:\n",
    "            vec_store = Chroma.from_documents(batch, embeddings, persist_directory=persist_directory)\n",
    "        else:\n",
    "            vec_store.add_documents(batch)\n",
    "        time.sleep(1.0)\n",
    "    vec_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8wCr8FuFnUf"
   },
   "outputs": [],
   "source": [
    "vec_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl4Zh4V5FqEM"
   },
   "outputs": [],
   "source": [
    "ret_docs = vec_store.similarity_search_with_score(\n",
    "    \"nuclear power\", \n",
    "    k=3, \n",
    "    filter={\"source\": \"https://www.congress.gov/bill/118th-congress/house-concurrent-resolution/17\"},\n",
    ")\n",
    "\n",
    "print(\"number of returned docs: \", len(ret_docs))\n",
    "for doc in ret_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdr41mxrb-rn"
   },
   "source": [
    "# Part 3 - Build A RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bllOTmFfWdS"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# base classes to examine\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.schema import BaseRetriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Retriever from Chroma VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBt_jRxmF94T"
   },
   "outputs": [],
   "source": [
    "retriever = vec_store.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ODHd9ltGQlq"
   },
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an LLM\n",
    "\n",
    "With LangChain we can use a text completion model or a chat model for QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROVIDER == \"HF\":\n",
    "    # https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=CONFIG[\"llm\"],\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0,\n",
    "            \"max_length\": 128,\n",
    "            \"top_p\": 0.95,\n",
    "            \"repetition_penalty\": 5.0,\n",
    "        })\n",
    "elif PROVIDER == \"OPENAI\":\n",
    "    if CONFIG[\"llm\"].startswith(\"text\"):\n",
    "        llm = OpenAI(model_name=CONFIG[\"llm\"], temperature=0)\n",
    "    elif CONFIG[\"llm\"].startswith(\"gpt\"):\n",
    "        llm = ChatOpenAI(model_name=CONFIG[\"llm\"], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPEKaYi3GmE1"
   },
   "outputs": [],
   "source": [
    "# create a RetrievalQA Chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XukLCIikWVp"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "query = \"What are the primary themes around energy policy?\"\n",
    "with get_openai_callback() as cb:\n",
    "    out = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in out['source_documents']:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = qa(\"What is the solution to climate change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in out['source_documents']:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* what are the components of the RetrievalQA chain?\n",
    "* what is the QA prompt?\n",
    "* how would you modify the QA prompt?\n",
    "* what is the difference between the following qa chain types?,\n",
    "    * stuff\n",
    "    * map_reduce\n",
    "    * map_rerank\n",
    "    * refine\n",
    " \n",
    "## Resources\n",
    "\n",
    "* https://python.langchain.com/docs/modules/chains/document/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseCombineDocumentsChain?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CombineDocumentChains\n",
    "\n",
    "* https://python.langchain.com/docs/modules/chains/document/stuff\n",
    "* https://python.langchain.com/docs/modules/chains/document/refine\n",
    "* https://python.langchain.com/docs/modules/chains/document/map_reduce\n",
    "* https://python.langchain.com/docs/modules/chains/document/map_rerank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8MdA3SOznZkm"
   },
   "source": [
    "## Examine the RetrievalQA Prompt \n",
    "\n",
    "Note that the prompt template used will depend on the choice of LLM (text completion vs chat). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhmxZhEIbbpR"
   },
   "outputs": [],
   "source": [
    "prompt_template = qa.combine_documents_chain.llm_chain.prompt\n",
    "print(prompt_template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cR-f8EBgjwOi"
   },
   "source": [
    "# Part 4 - Create a RetrievalQAWithSourcesChain\n",
    "\n",
    "Now we will do the same thing using a chain that provides sources in the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GDnvwb2jzxy"
   },
   "outputs": [],
   "source": [
    "qaws = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwOMdmEmj5qE"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "out = qaws(\"What is the solution to climate change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGtEw6NVj_nj"
   },
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['sources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flHelL8nkLEy"
   },
   "outputs": [],
   "source": [
    "print(out['source_documents'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity \n",
    "\n",
    "* In this example, all of the returned document chunks came from one original document (118HCONRES37). What can be done to encourage a more diverse set of documents?\n",
    "* What prompt is used? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIBbG6f6BFC5"
   },
   "outputs": [],
   "source": [
    "pt = qaws.combine_documents_chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B90CgFIDBDmM"
   },
   "outputs": [],
   "source": [
    "print(pt.format(summaries='[SUMMARIES]', question='[QUESTION]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxLyw-PlJIMQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
