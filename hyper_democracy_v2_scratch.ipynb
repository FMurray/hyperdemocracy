{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FMurray/hyperdemocracy/blob/main/hyper_democracy_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KkXxD28y6u2w"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on a google colab, uncomment the lines below to fetch the requirements file and the hyperdemocracy.py module\n",
    "# and pip install the requirements\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/FMurray/hyperdemocracy/main/requirements.txt\n",
    "#!wget https://raw.githubusercontent.com/FMurray/hyperdemocracy/main/hyperdemocracy.py\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Formatted Output\n",
    "\n",
    "Note that we patch the builtin Python `print` function with `rich.print` in the cell below. If you prefer a more traditional print output you can comment out the import below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Cost\n",
    "\n",
    "Running this notebook with your OpenAI key in an environment variable will charge a small amount of money to your OpenAI account. The total cost of running this notebook multiple times should be less than 10 cents but that can change if the datasource is changed. Each cell that makes a request to an OpenAI endpoint that costs money will have the following comment in it, \n",
    "\n",
    "```\n",
    "## THIS CELL SPENDS MONEY ##\n",
    "```\n",
    "\n",
    "Up to date pricing information on OpenAI models can be found here https://openai.com/pricing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqdS7Elhfpl"
   },
   "source": [
    "# Setup Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use local secrets, add a file called .env to this directory and uncomment the lines below\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "#%dotenv ./.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtSzpF4Ehii5"
   },
   "outputs": [],
   "source": [
    "# if you are using google colab, uncomment the lines below to manually enter your OpenAI key.\n",
    "\n",
    "#import getpass\n",
    "#os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vSe6gY8_gazO"
   },
   "source": [
    "# Load Assembleco Records\n",
    "\n",
    "We are going to use a small subset of records provided by https://assembled.app/.\n",
    "\n",
    "For the purposes of this workshop, we have created a [huggingface dataset](https://huggingface.co/datasets/assembleco/hyperdemocracy)  which we can load using the `load_dataset` function. This is all handled for you in the `load_assembleco_records` function. See more info here [datasets](https://huggingface.co/docs/datasets/index) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperdemocracy import load_assembleco_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_assembleco_records(process=True, strip_html=True, remove_empty_body=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5r6oJ7MmQM5"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4eSYiO5Ii9X"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-HUxJ4XAFOaZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sponsor Graph Sidequest\n",
    "\n",
    "We will be focusing on the text content of the legislation in this workshop, but if you would like to explore building a graph from the sponsor / co-sponsor / legislation network check out the [sponsor_graph notebook](https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/sponsor_graph.ipynb) to get started."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5GM8DqH7Ojeu"
   },
   "source": [
    "# From Pandas Dataframe to LangChain Documents\n",
    "\n",
    "A langchain document is a simple class with two attributes, \n",
    "* page_content (a string)\n",
    "* metadata (a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH58_PTsRJyD"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take each row from our legislation DataFrame and create a LangChain Document. We use the `body` column for the `page_content` attribute and populate the `metadata` attribute with data from some of the other columns. Note that the `source` key in the `metadata` dictionary is associated with a congress.gov url. The `source` key can hold an arbitrary string and will become important when we look into question answering systems that return information about the sources used to answer a question. We also restrict ourselves to `str`, `int`, and `float` types in the other values of our `metadata` dictionary. This is to make it easy to use them as filters when querying our vectorstore. If that doesn't make sense, dont worry! It will by the end of the workshop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLHx06mHepKQ"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for irow, row in df.iterrows():\n",
    "    doc = Document(\n",
    "        page_content=row['body'],\n",
    "        metadata={\n",
    "            # Note: chroma can only filter on float, str, or int\n",
    "            # https://docs.trychroma.com/usage-guide#using-where-filters\n",
    "            'key': row['key'],\n",
    "            'congress_num': row['congress_num'],\n",
    "            'legis_class': row['legis_class'],\n",
    "            'legis_num': row['legis_num'],\n",
    "            'name': row['name'],\n",
    "            'summary': row['summary'],\n",
    "            'sponsor': row['sponsors'][0][0],\n",
    "            'source': row['congress_gov_url'],\n",
    "        },\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-fFRGsTUDGX"
   },
   "outputs": [],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* examine the Document content\n",
    "* visit the congress.gov URL and view the document in various formats\n",
    "* examine the body text below\n",
    "* read the summary of the document and attempt to connect it with the long form text of the document\n",
    "* see if there is anything interesting that can be done with this https://twitter.com/RLanceMartin/status/1670489431168659456?s=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fceFBIiDRcZk"
   },
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siRiWjCyj63F"
   },
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pVPwxKHLA3fC"
   },
   "source": [
    "# Document QA Quickstart\n",
    "\n",
    "* https://python.langchain.com/docs/modules/chains/additional/question_answering\n",
    "* https://python.langchain.com/docs/modules/chains/document.html\n",
    "\n",
    "Our goal is to setup a question answering (QA) system that can repond to natural language questions about legislation using source material that we provide. In the following section, we will explore the quickest most high-level approach provided by LangChain. Afterwards, we will unpack all of components and go over them in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBZermt7U07_"
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZyffFybguTs"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "index = VectorstoreIndexCreator().from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XukLCIikWVp"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "\n",
    "# QA \n",
    "\n",
    "# copy paste this cell and try some questions of your own\n",
    "query = \"What are the primary themes around energy policy?\"\n",
    "out = index.query(query)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UYqM62Kn3GS"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "\n",
    "# QA with sources\n",
    "\n",
    "# copy paste this cell and try some questions of your own\n",
    "out = index.query_with_sources(query)\n",
    "print(\"question:\\n\", out['question'])\n",
    "print(\"answer:\\n\", out['answer'])\n",
    "print(\"sources:\\n\", out['sources'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* Contemplate why the answers are slightly different between the \"QA\" result and the \"QA with sources\" result.\n",
    "* Visit the source links and check if the linked legislation is relevant to the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4ry8zdrlCEqi"
   },
   "source": [
    "# Document QA - Step by Step\n",
    "\n",
    "Now that we've used the high-level tools of LangChain, lets go into more detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KCdDA0eOXJ_1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Langchain Text Splitters\n",
    "\n",
    "> When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "> At a high level, text splitters work as following:\n",
    "\n",
    ">    1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    ">    2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    ">    3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "> That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    ">    1. How the text is split\n",
    ">    2. How the chunk size is measured\n",
    "\n",
    "-- https://python.langchain.com/docs/modules/data_connection/document_transformers/#text-splitters\n",
    "\n",
    "Here are some useful options for splitting legislative text, \n",
    "\n",
    "* [character text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)\n",
    "  * How the text is split: by single character\n",
    "  * How the chunk size is measured: by number of characters\n",
    "* [recursive text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
    "  * How the text is split: by list of characters\n",
    "  * How the chunk size is measured: by number of characters\n",
    "* [split by token](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token)\n",
    "  * How the text is split: by character passed in\n",
    "  * How the chunk size is measured: by tiktoken tokenizer\n",
    "\n",
    "If you are not familiar with the concept of a token, this article may help, \n",
    "* https://simonwillison.net/2023/Jun/8/gpt-tokenizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w6e8Bcia6gp"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"We hold these truths to be self-evident, that all men are created equal,\n",
    "\n",
    "that they are endowed by their Creator with certain unalienable Rights,\n",
    "\n",
    "that among these are Life, Liberty and the pursuit of Happiness.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the default separator\n",
    "CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=20, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what happens if we change the default separator\n",
    "CharacterTextSplitter(separator=\" \", chunk_size=20, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what overlap does\n",
    "CharacterTextSplitter(separator=\" \", chunk_size=20, chunk_overlap=10).split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the default separators\n",
    "RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=40, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \",\", \" \", \"\"], chunk_size=40, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length unit here is tokens not characters\n",
    "TokenTextSplitter(model_name=\"text-embedding-ada-002\", chunk_size=10, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length unit here is tokens not characters\n",
    "TokenTextSplitter(model_name=\"text-embedding-ada-002\", chunk_size=10, chunk_overlap=4).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [enc.decode_single_token_bytes(token) for token in enc.encode(text)]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Make a TextSplitter Choice here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCi59g8rFAMi"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da7F2FUuFR8q"
   },
   "outputs": [],
   "source": [
    "print(\"Number of original docs: \", len(docs))\n",
    "print(\"Number of split docs: \", len(split_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVOa0TebHtY"
   },
   "source": [
    "## Embed and Index Doc Chunks\n",
    "\n",
    "Now we will embed and index the document chunks from the previous section. \n",
    "We have many choices when it comes to text embedding models and vector indexes. \n",
    "For this tutorial we will choose, \n",
    "\n",
    "* text embedding model: `text_embedding_ada_002`\n",
    "* vector index:\n",
    "  * https://www.trychroma.com\n",
    "  * https://docs.trychroma.com/usage-guide#changing-the-distance-function\n",
    "  * https://github.com/nmslib/hnswlib/tree/master#supported-distances\n",
    "  * https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/chroma.py\n",
    "  * https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/utils.py#L10\n",
    "\n",
    "For a look at some of the top performing closed and open source text embedding models, check out the HuggingFace Massive Text Embedding Benchmark (MTEB), \n",
    "* https://huggingface.co/spaces/mteb/leaderboard\n",
    "  \n",
    "For a more detailed introduction to embeddings in general, see the embeddings notebook\n",
    "* https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/embeddings_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdBDoTvLbLfZ"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_documents([text])[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18-prZMJFisk"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "vec_store = Chroma.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8wCr8FuFnUf"
   },
   "outputs": [],
   "source": [
    "vec_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl4Zh4V5FqEM"
   },
   "outputs": [],
   "source": [
    "ret_docs = vec_store.similarity_search_with_score(\n",
    "    \"nuclear power\", \n",
    "    k=5, \n",
    "    filter={\"source\": \"https://www.congress.gov/bill/118th-congress/house-concurrent-resolution/17\"},\n",
    ")\n",
    "\n",
    "print(\"number of returned docs: \", len(ret_docs))\n",
    "for doc in ret_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdr41mxrb-rn"
   },
   "source": [
    "### Build A Retrieval Chain for Question Answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bllOTmFfWdS"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBt_jRxmF94T"
   },
   "outputs": [],
   "source": [
    "retriever = vec_store.as_retriever(search_kwargs={'k':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ODHd9ltGQlq"
   },
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPEKaYi3GmE1"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseCombineDocumentsChain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.combine_documents_chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fwzvjXPqbcrC"
   },
   "source": [
    "# Questions\n",
    "\n",
    "* what are the components of the RetrievalQA chain?\n",
    "* what is the QA prompt?\n",
    "* how would you modify the QA prompt?\n",
    "* what is the difference between the following qa chain types?,\n",
    "    * stuff\n",
    "    * map_reduce\n",
    "    * map_rerank\n",
    "    * refine\n",
    "\n",
    "# Resources\n",
    "\n",
    "* https://github.com/hwchase17/langchain/tree/master/langchain/chains/retrieval_qa\n",
    "* https://github.com/hwchase17/langchain/tree/master/langchain/chains/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(\"https://python.langchain.com/assets/images/stuff-818da4c66ee17911bc8861c089316579.jpg\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! Do not commit the outputs of this cell if it contains your API key\n",
    "\n",
    "rich.print(qa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8MdA3SOznZkm"
   },
   "source": [
    "## How many ways can we print a prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhmxZhEIbbpR"
   },
   "outputs": [],
   "source": [
    "prompt_template = qa.combine_documents_chain.llm_chain.prompt\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqSdtKYqjSwv"
   },
   "outputs": [],
   "source": [
    "print(prompt_template.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb-itdmsi_6n"
   },
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeBaZGNzigx6"
   },
   "outputs": [],
   "source": [
    "rich.print(prompt_template.format(context='[CONTEXT]', question='[QUESTION]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdGfg21jG3VA"
   },
   "outputs": [],
   "source": [
    "answer = qa(\"What is the solution to climate change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkaJ3v9CHPc7"
   },
   "outputs": [],
   "source": [
    "answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR-f8EBgjwOi"
   },
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GDnvwb2jzxy"
   },
   "outputs": [],
   "source": [
    "qaws = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! Do not commit the outputs of this cell if it contains your API key\n",
    "print(qaws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIBbG6f6BFC5"
   },
   "outputs": [],
   "source": [
    "pt = qaws.combine_documents_chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B90CgFIDBDmM"
   },
   "outputs": [],
   "source": [
    "print(pt.format(summaries='[SUMMARIES]', question='[QUESTION]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwOMdmEmj5qE"
   },
   "outputs": [],
   "source": [
    "answer = qaws(\"What is the solution to climate change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGtEw6NVj_nj"
   },
   "outputs": [],
   "source": [
    "answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flHelL8nkLEy"
   },
   "outputs": [],
   "source": [
    "print(answer['answer'])\n",
    "print(answer['sources'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Construction Sidequest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rPvFbf88HtOa"
   },
   "source": [
    "# TODO\n",
    "\n",
    "Try alternatives to stuff\n",
    "\n",
    "Figure out how to pass all the options to the high level constructor. \n",
    "\n",
    "https://github.com/hwchase17/langchain/blob/master/langchain/indexes/vectorstore.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7pVNm_wH0Aq"
   },
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Chroma, \n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4UtKAlz-f4u7"
   },
   "source": [
    "\n",
    "Sticking this here to decide if we want to use this in the course content\n",
    "\n",
    "https://xml.house.gov/\n",
    "\n",
    "TODO: Sidequest on implementing a langchain document loader using this XML schema ^^^\n",
    "\n",
    "https://www.everycrsreport.com/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J2yKjNzlGLSz"
   },
   "source": [
    "# Lets make it a conversation\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFKsRfVUOtvZ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkvuSKPqHVOb"
   },
   "outputs": [],
   "source": [
    "# TODO cover serializing the db to disk\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gq9o8v49HZjh"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqbSxH8THddW"
   },
   "outputs": [],
   "source": [
    "qachat = ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0), \n",
    "    db.as_retriever(), \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbgFkWm_Ho9Q"
   },
   "outputs": [],
   "source": [
    "query = \"What is the solution to climate change?\"\n",
    "answer = qachat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8baHeDdgIX2f"
   },
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaaP287rIdDC"
   },
   "outputs": [],
   "source": [
    "follow_up = \"How certain is the 350 number?\"\n",
    "result = qachat({\"question\": follow_up})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7UitxViI-mm"
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxLyw-PlJIMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
