{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FMurray/hyperdemocracy/blob/main/hyper_democracy_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KkXxD28y6u2w"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on a google colab, uncomment the lines below to fetch the requirements file and the hyperdemocracy.py module\n",
    "# and pip install the requirements\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/FMurray/hyperdemocracy/main/requirements.txt\n",
    "#!wget https://raw.githubusercontent.com/FMurray/hyperdemocracy/main/hyperdemocracy.py\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rich\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Formatted Output\n",
    "\n",
    "Note that we patch the builtin Python `print` function with `rich.print` in the cell below. If you prefer a more traditional print output you can comment out the import below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Cost\n",
    "\n",
    "Running this notebook with your OpenAI key in an environment variable will charge a small amount of money to your OpenAI account. The total cost of running this notebook multiple times should be less than $5 but that can change if the datasource is changed. Each cell that makes a request to an OpenAI endpoint that costs money will have the following comment in it, \n",
    "\n",
    "```\n",
    "## THIS CELL SPENDS MONEY ##\n",
    "```\n",
    "\n",
    "Up to date pricing information on OpenAI models can be found here https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DOLLARS_PER_1K_TOKENS = 0.0001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqdS7Elhfpl"
   },
   "source": [
    "# Setup Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use local secrets, add a file called .env to this directory and uncomment the lines below\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtSzpF4Ehii5"
   },
   "outputs": [],
   "source": [
    "# if you are using google colab, uncomment the lines below to manually enter your OpenAI key.\n",
    "\n",
    "#import getpass\n",
    "#os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for development\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vSe6gY8_gazO"
   },
   "source": [
    "# Load Assembleco Records\n",
    "\n",
    "We are going to use a small subset of records provided by https://assembled.app/.\n",
    "\n",
    "For the purposes of this workshop, we have created a [huggingface dataset](https://huggingface.co/datasets/assembleco/hyperdemocracy)  which we can load using the `load_dataset` function. This is all handled for you in the `load_assembleco_records` function. See more info here [datasets](https://huggingface.co/docs/datasets/index) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperdemocracy import load_assembleco_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_assembleco_records(process=True, strip_html=True, remove_empty_body=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5r6oJ7MmQM5"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4eSYiO5Ii9X"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-HUxJ4XAFOaZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sponsor Graph Sidequest\n",
    "\n",
    "We will be focusing on the text content of the legislation in this workshop, but if you would like to explore building a graph from the sponsor / co-sponsor / legislation network check out the [sponsor_graph notebook](https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/sponsor_graph.ipynb) to get started."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5GM8DqH7Ojeu"
   },
   "source": [
    "# From Pandas Dataframe to LangChain Documents\n",
    "\n",
    "A langchain document is a simple class with two attributes, \n",
    "* page_content (a string)\n",
    "* metadata (a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH58_PTsRJyD"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take each row from our legislation DataFrame and create a LangChain Document. We use the `body` column for the `page_content` attribute and populate the `metadata` attribute with data from some of the other columns. Note that the `source` key in the `metadata` dictionary is associated with a congress.gov url. The `source` key can hold an arbitrary string and will become important when we look into question answering systems that return information about the sources used to answer a question. We also restrict ourselves to `str`, `int`, and `float` types in the other values of our `metadata` dictionary. This is to make it easy to use them as filters when querying our vectorstore. If that doesn't make sense, dont worry! It will by the end of the workshop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLHx06mHepKQ"
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for irow, row in df.iterrows():\n",
    "    doc = Document(\n",
    "        page_content=row['body'],\n",
    "        metadata={\n",
    "            # Note: chroma can only filter on float, str, or int\n",
    "            # https://docs.trychroma.com/usage-guide#using-where-filters\n",
    "            'key': row['key'],\n",
    "            'congress_num': row['congress_num'],\n",
    "            'legis_class': row['legis_class'],\n",
    "            'legis_num': row['legis_num'],\n",
    "            'name': row['name'],\n",
    "            'summary': row['summary'],\n",
    "            'sponsor': row['sponsors'][0][0],\n",
    "            'source': row['congress_gov_url'],\n",
    "        },\n",
    "    )\n",
    "    all_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-fFRGsTUDGX"
   },
   "outputs": [],
   "source": [
    "print(all_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* examine the Document content\n",
    "* visit the congress.gov URL and view the document in various formats\n",
    "* examine the body text below\n",
    "* read the summary of the document and attempt to connect it with the long form text of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fceFBIiDRcZk"
   },
   "outputs": [],
   "source": [
    "print(all_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siRiWjCyj63F"
   },
   "outputs": [],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsample Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = all_docs[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pVPwxKHLA3fC"
   },
   "source": [
    "# Document QA Quickstart\n",
    "\n",
    "* https://python.langchain.com/docs/modules/chains/additional/question_answering\n",
    "* https://python.langchain.com/docs/modules/chains/document.html\n",
    "\n",
    "Our goal is to setup a question answering (QA) system that can repond to natural language questions about legislation using source material that we provide. In the following section, we will explore the quickest most high-level approach provided by LangChain. Afterwards, we will unpack all of components and go over them in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBZermt7U07_"
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_docs(docs):\n",
    "    num_tokens = 0\n",
    "    enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "    for doc in docs:\n",
    "        num_tokens += len(enc.encode(doc.page_content))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens_in_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate embedding cost\n",
    "num_tokens = count_tokens_in_docs(docs)\n",
    "cost = EMBED_DOLLARS_PER_1K_TOKENS * num_tokens / 1000\n",
    "print('Tokens Used: ', num_tokens)\n",
    "print('Total Cost (USD): ', '$'+str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZyffFybguTs"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "\n",
    "# note that the get_openai_callback doesn't support embedding models yet\n",
    "# https://github.com/hwchase17/langchain/issues/945\n",
    "# maybe someone here will change this! \n",
    "with get_openai_callback() as cb:\n",
    "    index = VectorstoreIndexCreator().from_documents(docs)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use this to show that the default embedding model is `text-embedding-ada-002` \n",
    "#index.vectorstore._embedding_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XukLCIikWVp"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "\n",
    "# QA \n",
    "\n",
    "# copy paste this cell and try some questions of your own\n",
    "query = \"What are the primary themes around energy policy?\"\n",
    "with get_openai_callback() as cb:\n",
    "    out = index.query(query)\n",
    "    print(out)\n",
    "    print()\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UYqM62Kn3GS"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "\n",
    "# QA with sources\n",
    "\n",
    "# copy paste this cell and try some questions of your own\n",
    "with get_openai_callback() as cb:\n",
    "    out = index.query_with_sources(query)\n",
    "    print(\"question:\\n\", out['question'])\n",
    "    print(\"answer:\\n\", out['answer'])\n",
    "    print(\"sources:\")\n",
    "    for source in out['sources'].split():\n",
    "        print(source.strip())\n",
    "    print()\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* Contemplate why the answers are slightly different between the \"QA\" result and the \"QA with sources\" result.\n",
    "* Visit the source links and check if the linked legislation is relevant to the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4ry8zdrlCEqi"
   },
   "source": [
    "# Document QA - Step by Step\n",
    "\n",
    "Now that we've used the high-level tools of LangChain, lets go into more detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KCdDA0eOXJ_1"
   },
   "source": [
    "# Part 1 - Langchain Text Splitters\n",
    "\n",
    "> When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "> At a high level, text splitters work as following:\n",
    "\n",
    ">    1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    ">    2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    ">    3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "> That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    ">    1. How the text is split\n",
    ">    2. How the chunk size is measured\n",
    "\n",
    "-- https://python.langchain.com/docs/modules/data_connection/document_transformers/#text-splitters\n",
    "\n",
    "Here are some useful options for splitting legislative text, \n",
    "\n",
    "* [character text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)\n",
    "  * How the text is split: by single character\n",
    "  * How the chunk size is measured: by number of characters\n",
    "* [recursive text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
    "  * How the text is split: by list of characters\n",
    "  * How the chunk size is measured: by number of characters\n",
    "* [split by token](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token)\n",
    "  * How the text is split: by character passed in\n",
    "  * How the chunk size is measured: by tiktoken tokenizer\n",
    "\n",
    "If you are not familiar with the concept of a token, this article may help, \n",
    "* https://simonwillison.net/2023/Jun/8/gpt-tokenizers/\n",
    "\n",
    "Mini Side Quest\n",
    "* see if there is anything interesting that can be done with this https://twitter.com/RLanceMartin/status/1670489431168659456?s=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w6e8Bcia6gp"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"We hold these truths to be self-evident, that all men are created equal,\n",
    "\n",
    "that they are endowed by their Creator with certain unalienable Rights,\n",
    "\n",
    "that among these are Life, Liberty and the pursuit of Happiness.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the default separator\n",
    "CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=20, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what happens if we change the default separator\n",
    "CharacterTextSplitter(separator=\" \", chunk_size=20, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what overlap does\n",
    "CharacterTextSplitter(separator=\" \", chunk_size=20, chunk_overlap=10).split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the default separators\n",
    "RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=40, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what happens if we add \",\" to the separators\n",
    "RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \",\", \" \", \"\"], chunk_size=40, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length unit for chunk_size is now tokens not characters\n",
    "TokenTextSplitter(model_name=\"text-embedding-ada-002\", chunk_size=10, chunk_overlap=0).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for chunk_overlap\n",
    "TokenTextSplitter(model_name=\"text-embedding-ada-002\", chunk_size=10, chunk_overlap=4).split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that \"self-evident\" is 4 tokens\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "print(enc)\n",
    "token_ids = enc.encode(\"self-evident\")\n",
    "token_strs = [enc.decode_single_token_bytes(token) for token in token_ids]\n",
    "print(\"token_ids: \", token_ids)\n",
    "print(\"token_strs: \", token_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Make a TextSplitter Choice here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCi59g8rFAMi"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da7F2FUuFR8q"
   },
   "outputs": [],
   "source": [
    "print(\"Number of original docs: \", len(docs))\n",
    "print(\"Number of split docs: \", len(split_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVOa0TebHtY"
   },
   "source": [
    "# Part 2 - Embed and Index Doc Chunks\n",
    "\n",
    "Now we will embed and index the document chunks from the previous section. \n",
    "We have many choices when it comes to text embedding models and vector indexes. \n",
    "For this tutorial we will choose, \n",
    "\n",
    "* text embedding model: `text_embedding_ada_002`\n",
    "* vector index:\n",
    "  * https://www.trychroma.com\n",
    "  * https://docs.trychroma.com/usage-guide#changing-the-distance-function\n",
    "  * https://github.com/nmslib/hnswlib/tree/master#supported-distances\n",
    "  * https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/chroma.py\n",
    "  * https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/utils.py#L10\n",
    "\n",
    "For a look at some of the top performing closed and open source text embedding models, check out the HuggingFace Massive Text Embedding Benchmark (MTEB), \n",
    "* https://huggingface.co/spaces/mteb/leaderboard\n",
    "  \n",
    "For a more detailed introduction to embeddings in general, see the embeddings notebook\n",
    "* https://github.com/FMurray/hyperdemocracy/blob/main/sidequests/embeddings_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdBDoTvLbLfZ"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "persist_directory = \"hyperdemocracy-chromadb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18-prZMJFisk"
   },
   "outputs": [],
   "source": [
    "## THIS CELL SPENDS MONEY ##\n",
    "vec_store = Chroma.from_documents(split_docs, embeddings, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8wCr8FuFnUf"
   },
   "outputs": [],
   "source": [
    "vec_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save vectorstore to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload vectorstore from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = None\n",
    "vec_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl4Zh4V5FqEM"
   },
   "outputs": [],
   "source": [
    "ret_docs = vec_store.similarity_search_with_score(\n",
    "    \"nuclear power\", \n",
    "    k=5, \n",
    "    filter={\"source\": \"https://www.congress.gov/bill/118th-congress/house-concurrent-resolution/17\"},\n",
    ")\n",
    "\n",
    "print(\"number of returned docs: \", len(ret_docs))\n",
    "for doc in ret_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdr41mxrb-rn"
   },
   "source": [
    "# Part 3 - Build A RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bllOTmFfWdS"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# base classes to examine\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.schema import BaseRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Retriever from Chroma VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBt_jRxmF94T"
   },
   "outputs": [],
   "source": [
    "retriever = vec_store.as_retriever(search_kwargs={'k':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ODHd9ltGQlq"
   },
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an LLM\n",
    "\n",
    "With LangChain we can use a text completion model or a chat model for QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line to use a text completion model\n",
    "#llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line to use a chat model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPEKaYi3GmE1"
   },
   "outputs": [],
   "source": [
    "# create a RetrievalQA Chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa(\"What is the solution to climate change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we set `return_source_documents=True` and k=10\n",
    "# we can see the 10 document chunks that were returned\n",
    "print(answer['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "* what are the components of the RetrievalQA chain?\n",
    "* what is the QA prompt?\n",
    "* how would you modify the QA prompt?\n",
    "* what is the difference between the following qa chain types?,\n",
    "    * stuff\n",
    "    * map_reduce\n",
    "    * map_rerank\n",
    "    * refine\n",
    " \n",
    "## Resources\n",
    "\n",
    "* https://python.langchain.com/docs/modules/chains/document/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseCombineDocumentsChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CombineDocumentChains\n",
    "\n",
    "* https://python.langchain.com/docs/modules/chains/document/stuff\n",
    "* https://python.langchain.com/docs/modules/chains/document/refine\n",
    "* https://python.langchain.com/docs/modules/chains/document/map_reduce\n",
    "* https://python.langchain.com/docs/modules/chains/document/map_rerank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8MdA3SOznZkm"
   },
   "source": [
    "## Examine the RetrievalQA Prompt \n",
    "\n",
    "Note that the prompt template used will depend on the choice of LLM (text completion vs chat). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhmxZhEIbbpR"
   },
   "outputs": [],
   "source": [
    "prompt_template = qa.combine_documents_chain.llm_chain.prompt\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR-f8EBgjwOi"
   },
   "source": [
    "# Part 4 - Create a RetrievalQAWithSourcesChain\n",
    "\n",
    "Now we will do the same thing using a chain that provides sources in the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GDnvwb2jzxy"
   },
   "outputs": [],
   "source": [
    "qaws = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwOMdmEmj5qE"
   },
   "outputs": [],
   "source": [
    "answer = qaws(\"What is the solution to climate change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGtEw6NVj_nj"
   },
   "outputs": [],
   "source": [
    "answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['sources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flHelL8nkLEy"
   },
   "outputs": [],
   "source": [
    "print(answer['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity \n",
    "\n",
    "* In this example, all of the returned document chunks came from one original document (118HCONRES37). What can be done to encourage a more diverse set of documents?\n",
    "* What prompt is used? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIBbG6f6BFC5"
   },
   "outputs": [],
   "source": [
    "pt = qaws.combine_documents_chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B90CgFIDBDmM"
   },
   "outputs": [],
   "source": [
    "print(pt.format(summaries='[SUMMARIES]', question='[QUESTION]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxLyw-PlJIMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
